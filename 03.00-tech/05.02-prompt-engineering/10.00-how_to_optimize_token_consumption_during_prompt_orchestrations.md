---
title: "How to Optimize Token Consumption During Prompt Orchestrations"
author: "Dario Airoldi"
date: "2026-01-25"
categories: [tech, github-copilot, prompt-engineering, optimization, token-efficiency]
description: "Master token optimization strategies for multi-agent workflows: context reduction, deterministic tools, semantic caching, and provider caching to reduce costs by up to 90%"
---

# How to Optimize Token Consumption During Prompt Orchestrations

Token consumption is <mark>**the hidden cost driver**</mark> in complex AI workflows.  
While the previous article explored *how* information flows between prompts, agents, and tools, this article focuses on *minimizing* the tokens consumed during that flowâ€”directly reducing costs, latency, and context window pressure.

You'll learn **nine distinct optimization strategies** organized into three categories that can be combined for maximum effect:

**Input Optimization:**

1. **Context reduction** â€” Minimizing what enters the context window
2. **Provider prompt caching** â€” Leveraging built-in provider caching mechanisms
3. **Semantic caching** â€” Reusing results for semantically similar queries

**Processing Optimization:**

4. **Model selection** â€” Using smaller models for simpler tasks
5. **Batch processing** â€” Async processing with 50% cost discount
6. **Request consolidation** â€” Combining sequential steps into single requests

**Output Optimization:**

7. **Output token reduction** â€” Generating fewer tokens
8. **Deterministic tools** â€” Bypassing AI entirely for predictable operations
9. **Streaming and parallelization** â€” Reducing perceived latency and enabling speculative execution  

By applying these strategies, you can achieve <mark>**50-90% cost reduction**</mark> while maintaining or improving response quality.

## Table of Contents

- [ğŸ¯ Why token optimization matters](#-why-token-optimization-matters)
- [ğŸ“Š Token consumption anatomy](#-token-consumption-anatomy)
- [âœ‚ï¸ Strategy 1: Context reduction](#ï¸-strategy-1-context-reduction)
- [ğŸ’¾ Strategy 2: Provider prompt caching](#-strategy-2-provider-prompt-caching)
- [ğŸ§  Strategy 3: Semantic caching](#-strategy-3-semantic-caching)
- [ğŸšï¸ Strategy 4: Model selection](#ï¸-strategy-4-model-selection)
- [ğŸ“¦ Strategy 5: Batch processing](#-strategy-5-batch-processing)
- [ğŸ”— Strategy 6: Request consolidation](#-strategy-6-request-consolidation)
- [ğŸ“‰ Strategy 7: Output token reduction](#-strategy-7-output-token-reduction)
- [âš™ï¸ Strategy 8: Deterministic tools](#ï¸-strategy-8-deterministic-tools)
- [âš¡ Strategy 9: Streaming and parallelization](#-strategy-9-streaming-and-parallelization)
- [ğŸ“ˆ Strategy comparison matrix](#-strategy-comparison-matrix)
- [ğŸ”§ Implementation patterns](#-implementation-patterns)
- [âš ï¸ Common pitfalls](#ï¸-common-pitfalls)
- [ğŸ¯ Conclusion](#-conclusion)
- [ğŸ“š References](#-references)

---

# ğŸ¯ Why token optimization matters

## The cost equation

Every interaction with an AI model consumes tokens from three sources:

| Source | Description | Cost Impact |
|--------|-------------|-------------|
| **<mark>Input tokens</mark>** | Context window content (prompts, instructions, context) | <mark>Paid per request</mark> |
| **<mark>Output tokens</mark>** | Model-generated response | <mark>Paid per request (typically 3-5Ã— input cost)</mark> |
| **<mark>Reasoning tokens</mark>** | Internal reasoning (o3, o4-mini, extended thinking) | <mark>Hidden cost, often 10-50Ã— visible output</mark> |

### The multiplication problem

In multi-agent orchestrations, token consumption isn't additiveâ€”it's <mark>**multiplicative**</mark>:

```
Single request:     1,000 input + 500 output = 1,500 tokens

5-phase workflow with full context transfer:
â”œâ”€â”€ Phase 1:  1,000 + 500 = 1,500 tokens
â”œâ”€â”€ Phase 2:  1,500 + 800 = 2,300 tokens (inherited context)
â”œâ”€â”€ Phase 3:  2,300 + 600 = 2,900 tokens
â”œâ”€â”€ Phase 4:  2,900 + 700 = 3,600 tokens
â”œâ”€â”€ Phase 5:  3,600 + 400 = 4,000 tokens
â”‚
â””â”€â”€ TOTAL: 14,300 tokens (9.5Ã— single request)
```

Without optimization, a 5-phase workflow consumes nearly <mark>**10Ã— the tokens**</mark> of a single interaction.

### Real-world cost impact

| Scenario | Unoptimized | Optimized | Savings |
|----------|-------------|-----------|---------|
| Simple 3-phase workflow | ~8,000 tokens | ~3,000 tokens | 62% |
| Complex 6-phase orchestration | ~45,000 tokens | ~12,000 tokens | 73% |
| Validation pipeline (10 articles) | ~200,000 tokens | ~40,000 tokens | 80% |
| Daily development workflow | ~500,000 tokens | ~100,000 tokens | 80% |

At typical API pricing, 80% savings translates to substantial cost reduction over time.

---

# ğŸ“Š Token consumption anatomy

Before optimizing, understand <mark>where tokens are consumed</mark> in a typical prompt orchestration:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CONTEXT WINDOW BREAKDOWN                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  SYSTEM CONTEXT (~2,000-5,000 tokens)                            â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Agent definition (.agent.md)          ~800-1,500 tokens     â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Instructions (.instructions.md)       ~500-2,000 tokens     â”‚  â”‚
â”‚  â”‚  â””â”€â”€ Global instructions (copilot-instructions.md) ~500-1,000    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  USER CONTEXT (~500-10,000 tokens)                               â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Prompt file content                   ~300-2,000 tokens     â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Included snippets (#file:...)         ~200-2,000 tokens     â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ User message                          ~50-500 tokens        â”‚  â”‚
â”‚  â”‚  â””â”€â”€ Handoff context (from previous agent) ~0-5,000 tokens       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  TOOL RESULTS (~0-50,000 tokens)                                 â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ File reads (read_file)                ~500-5,000 each       â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Search results (semantic_search)      ~1,000-3,000          â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ Web fetches (fetch_webpage)           ~3,000-15,000         â”‚  â”‚
â”‚  â”‚  â””â”€â”€ MCP tool results                      ~500-10,000           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  CONVERSATION HISTORY (~0-100,000 tokens)                        â”‚  â”‚
â”‚  â”‚  â””â”€â”€ Prior turns in multi-turn conversation                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Optimization opportunities by category

| Category | Typical % of Context | Optimization Strategy |
|----------|---------------------|----------------------|
| System context | 5-15% | Instruction file pruning, agent streamlining |
| User context | 5-20% | Prompt compression, snippet elimination |
| Tool results | 20-60% | <mark>**Targeted reads, result limits, deterministic tools**</mark> |
| Conversation history | 10-50% | <mark>**Progressive summarization, file-based isolation**</mark> |

<mark>**Key insight**</mark>: Tool results and conversation history are the biggest optimization targets. Focus there first.

---

# âœ‚ï¸ Strategy 1: Context reduction

<mark>**Context reduction**</mark> minimizes what enters the context window in the first placeâ€”the most direct path to token savings.

## Technique 1.1: Targeted file reads

âŒ **Wasteful pattern**:
```markdown
Read the entire file for context.
```

This often results in reading 500+ lines when only 20 lines are relevant.

âœ… **Efficient pattern**:
```markdown
Read lines 45-65 of the configuration file where the validation logic is defined.
```

**Implementation in prompt files**:
```yaml
---
name: targeted-review
description: "Review specific section with minimal context"
tools: ['read_file']
---

## Process

1. Read ONLY the specific section:
   - For class definitions: read class header + target method (20-50 lines)
   - For configuration: read only relevant section
   - Use grep_search first to identify exact line ranges

2. **NEVER read entire files unless explicitly required**
```

### Token savings example

| Approach | Tokens Consumed | Savings |
|----------|-----------------|---------|
| Read entire 500-line file | ~5,000 tokens | â€” |
| Read targeted 50 lines | ~500 tokens | <mark>**90%**</mark> |
| Use grep to find, then read 20 lines | ~200 tokens | <mark>**96%**</mark> |

## Technique 1.2: Search result limiting

âŒ **Wasteful pattern**:
```markdown
Search the codebase for validation patterns.
```

This can return 20+ results, each consuming hundreds of tokens.

âœ… **Efficient pattern**:
```markdown
Search for validation patterns, limit to 3 most relevant results.
```

**In prompts**:
```yaml
---
tools: ['semantic_search', 'grep_search']
---

## Tool Usage Guidelines

When searching:
1. **Use grep_search first** for known patterns (cheaper, faster)
2. **Limit semantic_search results** to 3-5 maximum
3. **Be specific in queries** â€” "authentication middleware" not "middleware"
```

## Technique 1.3: Progressive summarization

Instead of passing full conversation history between phases, <mark>compress it to essential data</mark>:

```markdown
## Phase Completion Template

When completing this phase, produce a **PHASE SUMMARY** (max 200 tokens):

### Phase {N} Summary

**Decisions Made**:
- [Decision 1]: [Rationale]
- [Decision 2]: [Rationale]

**Critical Outputs**:
- [Output file/artifact]: [1-line description]

**For Next Phase**:
- [Specific instruction for successor]

---
<!-- Full details are in: [file reference] -->
```

### Token savings from summarization

| Handoff Method | Tokens per Handoff | 5-Phase Total |
|----------------|-------------------|---------------|
| Full context (`send: true`) | ~3,000 tokens | ~15,000 tokens |
| Progressive summary | ~300 tokens | ~1,500 tokens |
| **Savings** | | <mark>**90%**</mark> |

## Technique 1.4: Instruction file pruning

Instructions files apply automatically based on `applyTo` patterns. <mark>Overly broad patterns waste tokens</mark>:

âŒ **Wasteful**:
```yaml
---
applyTo: "**/*"   # Applies to EVERY file
---
# 200 lines of React component guidelines...
```

âœ… **Targeted**:
```yaml
---
applyTo: "**/*.tsx,**/*.jsx"   # Only React files
---
```

**Audit your instruction files**:
```powershell
# Find instruction files and their patterns
Get-ChildItem -Path ".github/instructions" -Filter "*.instructions.md" | 
    ForEach-Object { 
        $content = Get-Content $_.FullName -Raw
        if ($content -match 'applyTo:\s*"([^"]+)"') {
            [PSCustomObject]@{
                File = $_.Name
                Pattern = $matches[1]
            }
        }
    }
```

---

# ğŸ’¾ Strategy 2: Provider prompt caching

<mark>**Provider prompt caching**</mark> is automatic caching offered by LLM providers for repeated prompt prefixes. Unlike semantic caching, it's <mark>**exact-match based**</mark> and <mark>**built into the API**</mark>.

## How provider caching works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROVIDER PROMPT CACHING                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Request 1:                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Static Prefix (cached after 1024+ tokens)       â”‚ Dynamic Content  â”‚ â”‚
â”‚  â”‚ â€¢ System instructions                           â”‚ â€¢ User query     â”‚ â”‚
â”‚  â”‚ â€¢ Few-shot examples                             â”‚ â€¢ Context        â”‚ â”‚
â”‚  â”‚ â€¢ Tool definitions                              â”‚                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚            â–²                                                            â”‚
â”‚            â”‚ Cache write (1.25Ã— base price for Anthropic)               â”‚
â”‚                                                                         â”‚
â”‚  Request 2 (same prefix):                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Static Prefix (CACHE HIT - 0.1Ã— base price)     â”‚ New Dynamic      â”‚ â”‚
â”‚  â”‚                                                 â”‚ Content          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                         â”‚
â”‚  SAVINGS: 50-90% on cached prefix tokens                                â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Provider comparison

| Provider | Minimum Tokens | Cache Duration | Cache Read Cost | Cache Write Cost |
|----------|---------------|----------------|-----------------|------------------|
| **OpenAI** | 1,024 tokens | 5-60 min (in-memory) or 24h (extended) | <mark>**50% discount**</mark> | No extra cost |
| **Anthropic** | 1,024-4,096 tokens (model-dependent) | 5 min default, 1h optional | <mark>**90% discount**</mark> | 25% premium |
| **Google** | Varies | Context caching available | Reduced cost | Initial write cost |

> **ğŸ“ Anthropic model-specific minimums:**
> - Claude Sonnet 4/4.5, Opus 4/4.1: 1,024 tokens minimum
> - Claude Haiku 3.5, Opus 4.5: 4,096 tokens minimum
> - Claude Haiku 3: 2,048 tokens minimum
>
> **1-hour cache TTL:** For agent workflows or conversations where follow-up prompts may exceed 5 minutes, use `"ttl": "1h"` in the `cache_control` block (2Ã— base write price instead of 1.25Ã—).

> **âš ï¸ Cache invalidation:** The following changes invalidate cached content:
> - **Tool definitions:** Modifying names, descriptions, or parameters invalidates the entire cache
> - **Enabling/disabling features:** Web search, citations, or thinking toggles modify system prompts
> - **Images:** Adding or removing images anywhere in the prompt
> - **Content edits:** Any changes to cached prefix content require re-caching

## Optimization: Structure prompts for caching

The key principle: <mark>**Static content first, dynamic content last**</mark>.

âŒ **Cache-unfriendly structure**:
```yaml
## User Request
{{dynamic_user_input}}

## Instructions
[Static rules that could be cached but won't be]
```

âœ… **Cache-friendly structure**:
```yaml
## Identity
You are a senior code reviewer specializing in security analysis.

## Instructions
1. Check for injection vulnerabilities
2. Validate input sanitization
[... 500+ tokens of stable instructions ...]

## Examples
[... few-shot examples that rarely change ...]

## User Request
{{dynamic_content}} <!-- Only this part isn't cached -->
```

### Token savings from provider caching

| Scenario | Without Caching | With Caching | Savings |
|----------|-----------------|--------------|---------|
| First request (1,500 token prefix) | 1,500 tokens at full price | 1,500 tokens at 1.25Ã— (Anthropic) | -25% (write cost) |
| Subsequent requests (same prefix) | 1,500 tokens at full price | 1,500 tokens at 0.1Ã— (Anthropic) | <mark>**90%**</mark> |
| 10 requests with same prefix | 15,000 tokens | 1,875 + 1,350 = 3,225 tokens | <mark>**78%**</mark> |

---

# ğŸ§  Strategy 3: Semantic caching

<mark>**Semantic caching**</mark> stores and retrieves results for <mark>semantically similar queries</mark>â€”not just exact matches. This is powerful for:

- Repeated questions with slight variations
- Common patterns across projects
- Expensive operations (web fetches, large analyses)

## How semantic caching works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEMANTIC CACHE ARCHITECTURE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  User Query: "How do I authenticate users in Azure AD?"                 â”‚
â”‚                           â”‚                                             â”‚
â”‚                           â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  1. EMBEDDING GENERATION                                         â”‚   â”‚
â”‚  â”‚     Convert query â†’ vector embedding [0.23, -0.15, 0.87, ...]   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                             â”‚
â”‚                           â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  2. SIMILARITY SEARCH                                            â”‚   â”‚
â”‚  â”‚     Find cached queries with embedding distance < threshold      â”‚   â”‚
â”‚  â”‚     â€¢ "Azure AD user authentication" (0.92 similarity) âœ…        â”‚   â”‚
â”‚  â”‚     â€¢ "Set up OAuth in Azure" (0.78 similarity) âš ï¸               â”‚   â”‚
â”‚  â”‚     â€¢ "Install Azure CLI" (0.31 similarity) âŒ                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                             â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚           â”‚                               â”‚                            â”‚
â”‚           â–¼                               â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  CACHE HIT      â”‚            â”‚  CACHE MISS     â”‚                    â”‚
â”‚  â”‚  Return cached  â”‚            â”‚  Call LLM API   â”‚                    â”‚
â”‚  â”‚  response       â”‚            â”‚  Store result   â”‚                    â”‚
â”‚  â”‚  (0 tokens)     â”‚            â”‚  in cache       â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Implementation: GPTCache

[GPTCache](https://github.com/zilliztech/GPTCache) is an open-source semantic caching library:

```python
from gptcache import cache
from gptcache.adapter import openai

# Initialize semantic cache
cache.init()
cache.set_openai_key()

# Queries with similar embeddings hit cache
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "How to authenticate Azure AD?"}]
)
# Second query with similar meaning â†’ cache hit
response2 = openai.ChatCompletion.create(
    model="gpt-4", 
    messages=[{"role": "user", "content": "Azure AD authentication setup"}]
)
```

## When semantic caching helps vs. hurts

| âœ… Good Use Cases | âŒ Poor Use Cases |
|-------------------|-------------------|
| Repeated documentation queries | Unique, context-specific questions |
| Common coding patterns | Code with dynamic requirements |
| FAQ-style interactions | Creative/generative tasks |
| Reference lookups | Tasks requiring current state |

---

# ğŸšï¸ Strategy 4: Model selection

<mark>**Model selection**</mark> uses smaller, faster models for simpler tasksâ€”matching model capability to task complexity.

## The model selection principle

From OpenAI's latency optimization guide:

> "Smaller models usually run faster (and cheaper), and when used correctly can even outperform larger models."

## When to use which model size

| Task Complexity | Recommended Model | Token Cost |
|-----------------|-------------------|------------|
| **Simple classification** | GPT-3.5, Claude Haiku | <mark>**10-20Ã— cheaper**</mark> |
| **Structured extraction** | GPT-4o-mini, Haiku | <mark>**5-10Ã— cheaper**</mark> |
| **Code completion** | Fine-tuned smaller model | Variable |
| **Complex reasoning** | GPT-4, Claude Sonnet/Opus | Full price |
| **Creative/nuanced writing** | GPT-4, Claude Opus | Full price |

## Implementation: Task routing

```yaml
# orchestrator.agent.md
---
name: task-router
description: "Route tasks to appropriate model size"
---

## Task Routing Rules

### Use SMALLER model (GPT-3.5 / Claude Haiku) for:
- Simple classification (sentiment, category)
- Structured data extraction
- Reformatting/transformation
- Simple Q&A with clear answers

### Use LARGER model (GPT-4 / Claude Sonnet) for:
- Complex reasoning chains
- Nuanced analysis
- Creative content generation
- Multi-step problem solving
```

## Practical example: Split prompt by complexity

From OpenAI's latency guideâ€”split a single GPT-4 prompt into two:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BEFORE: Single GPT-4 request                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ GPT-4: Reasoning + Classification + Response Generation   â”‚  â”‚
â”‚  â”‚ Cost: $$$                                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AFTER: Split by complexity                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ GPT-3.5: Classification â”‚ â†’ â”‚ GPT-4: Response Generation  â”‚  â”‚
â”‚  â”‚ (cheap, fast)           â”‚   â”‚ (only when needed)          â”‚  â”‚
â”‚  â”‚ Cost: $                 â”‚   â”‚ Cost: $$                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  TOTAL: $$ (vs $$$ before)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Token savings from model selection

| Approach | Cost per 1M tokens | Savings |
|----------|-------------------|---------|
| All GPT-4 | ~$30 | â€” |
| GPT-3.5 for 70% of tasks | ~$10 | <mark>**67%**</mark> |
| Fine-tuned GPT-3.5 | ~$8 | <mark>**73%**</mark> |

---

# ğŸ“¦ Strategy 5: Batch processing

<mark>**Batch processing**</mark> submits multiple requests together for asynchronous processing, receiving a <mark>**50% cost discount**</mark> from both OpenAI and Anthropic.

## When to use batch processing

| âœ… Good Use Cases | âŒ Poor Use Cases |
|-------------------|-------------------|
| Bulk content analysis | Real-time chat interactions |
| Large-scale evaluations | Interactive coding assistance |
| Dataset classification | Time-sensitive operations |
| Content moderation pipelines | User-facing latency-sensitive features |

## Provider batch APIs

| Provider | Discount | Max Batch Size | Completion Time |
|----------|----------|----------------|-----------------|
| **OpenAI** | <mark>**50%**</mark> | 50,000 requests or 200MB | Within 24 hours |
| **Anthropic** | <mark>**50%**</mark> | 100,000 requests or 256MB | Within 24 hours (usually <1h) |

## Implementation example (Anthropic)

```python
import anthropic

client = anthropic.Anthropic()

# Create batch with multiple requests
batch = client.messages.batches.create(
    requests=[
        {
            "custom_id": "article-1",
            "params": {
                "model": "claude-sonnet-4-5",
                "max_tokens": 1024,
                "messages": [
                    {"role": "user", "content": "Analyze this article..."}
                ]
            }
        },
        {
            "custom_id": "article-2",
            "params": {
                "model": "claude-sonnet-4-5",
                "max_tokens": 1024,
                "messages": [
                    {"role": "user", "content": "Analyze this article..."}
                ]
            }
        }
        # ... up to 100,000 requests
    ]
)

# Poll for completion
while batch.processing_status != "ended":
    batch = client.messages.batches.retrieve(batch.id)
    time.sleep(60)

# Retrieve results
results = client.messages.batches.results(batch.id)
```

## Combining batch processing with prompt caching

Batch processing and prompt caching discounts <mark>**stack**</mark>:

> **âš ï¸ Important:** Cache hits in batch processing are **best-effort** due to asynchronous, concurrent processing. Anthropic reports typical cache hit rates of 30-98% depending on traffic patterns. To maximize hits: include identical `cache_control` blocks in every request, maintain steady request flow, and consider the 1-hour cache TTL for batch workloads.

| Optimization | Discount | Combined |
|--------------|----------|----------|
| Batch only | 50% | 50% |
| Cache read only | 90% | 90% |
| **Batch + Cache read** | 50% + 90% | <mark>**95%**</mark> |

### Token savings from batch processing

| Scenario | Standard API | Batch API | Savings |
|----------|-------------|-----------|---------|
| 1,000 article validations | $30 | $15 | <mark>**50%**</mark> |
| With prompt caching | $15 | $7.50 | <mark>**75%**</mark> (combined) |

---

# ğŸ”— Strategy 6: Request consolidation

<mark>**Request consolidation**</mark> combines multiple sequential LLM calls into a single request, eliminating round-trip latency and reducing total tokens.

## The consolidation principle

From OpenAI's latency guide:

> "Each time you make a request you incur some round-trip latency. If you have sequential steps for the LLM to perform, instead of firing off one request per step consider putting them in a single prompt."

## Before vs. After consolidation

âŒ **Before: Sequential requests**
```
Request 1: "Classify this text"           â†’ 200 tokens
Request 2: "Extract entities"             â†’ 300 tokens  
Request 3: "Summarize findings"           â†’ 400 tokens
                                          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL: 3 requests, ~900 tokens + 3Ã— round-trip latency
```

âœ… **After: Consolidated request**
```
Single Request: 
"Perform the following in sequence:
1. Classify this text
2. Extract entities
3. Summarize findings

Return results as JSON with keys: classification, entities, summary"

TOTAL: 1 request, ~600 tokens + 1Ã— round-trip latency
```

## Implementation pattern

```yaml
---
name: consolidated-analysis
description: "Multiple analysis steps in one request"
---

## Task

Perform ALL of the following analyses on the provided content:

1. **Classification**: Categorize the content type
2. **Entity Extraction**: Identify key entities (people, places, concepts)
3. **Sentiment Analysis**: Determine overall sentiment
4. **Summary**: Create a 2-sentence summary

## Output Format

Return a single JSON object:
```json
{
  "classification": "...",
  "entities": [...],
  "sentiment": "positive|neutral|negative",
  "summary": "..."
}
```
```

### Token savings from consolidation

| Approach | Requests | Tokens | Savings |
|----------|----------|--------|---------|
| Sequential (3 requests) | 3 | ~900 | â€” |
| Consolidated (1 request) | 1 | ~600 | <mark>**33% tokens + 66% latency**</mark> |

---

# ğŸ“‰ Strategy 7: Output token reduction

<mark>**Output token reduction**</mark> generates fewer response tokensâ€”often the highest-latency step in LLM processing.

## The output token principle

From OpenAI's latency guide:

> "Generating tokens is almost always the highest latency step when using an LLM: cutting 50% of your output tokens may cut ~50% your latency."

## Techniques for reducing output tokens

### Technique 7.1: Explicit brevity instructions

```markdown
## Response Guidelines

- Keep responses under 100 words
- Use bullet points, not paragraphs
- Omit pleasantries and preamble
- Answer directly, then stop
```

### Technique 7.2: Shortened JSON field names

âŒ **Verbose output** (~120 tokens):
```json
{
  "message_is_conversation_continuation": "True",
  "number_of_messages_in_conversation_so_far": "5",
  "user_sentiment": "Frustrated",
  "query_type": "Technical Support",
  "response_requirements": "Provide step-by-step troubleshooting"
}
```

âœ… **Compact output** (~50 tokens):
```json
{
  "cont": true,
  "n_msg": 5,
  "tone": "frustrated",
  "type": "tech_support",
  "reqs": "troubleshoot_steps"
}
```

### Technique 7.3: Use `max_tokens` and `stop_tokens`

```python
response = client.chat.completions.create(
    model="gpt-4",
    messages=[...],
    max_tokens=500,  # Hard limit on output
    stop=["\n\n", "---"]  # Stop at section breaks
)
```

### Token savings from output reduction

| Technique | Before | After | Savings |
|-----------|--------|-------|---------|
| Brevity instructions | ~500 tokens | ~150 tokens | <mark>**70%**</mark> |
| Shortened field names | ~120 tokens | ~50 tokens | <mark>**58%**</mark> |
| max_tokens limit | Variable | Capped | Predictable |

---

# âš™ï¸ Strategy 8: Deterministic tools

<mark>**Deterministic tools**</mark> bypass AI entirely for operations that don't require intelligenceâ€”they're <mark>**infinitely faster and consume zero tokens**</mark>.

## When to use deterministic tools

| Operation Type | Use AI? | Use Deterministic Tool? |
|----------------|---------|------------------------|
| Parse YAML frontmatter | âŒ | âœ… Regex/parser |
| Check if file exists | âŒ | âœ… File system check |
| Validate JSON schema | âŒ | âœ… JSON Schema validator |
| Count lines matching pattern | âŒ | âœ… grep + wc |
| Calculate hash/checksum | âŒ | âœ… Hash function |
| Compare timestamps | âŒ | âœ… Date comparison |
| **Analyze code semantics** | âœ… | âŒ |
| **Generate creative content** | âœ… | âŒ |
| **Make judgment calls** | âœ… | âŒ |

## Implementation: MCP server with deterministic tools

The key insight: <mark>wrap deterministic operations in MCP tools</mark> so agents can invoke them without AI processing.

**Example: Validation cache check (deterministic)**

```csharp
// IQPilot MCP Server - CheckValidationCache tool
[McpTool("check_validation_cache")]
public async Task<CacheCheckResult> CheckValidationCache(
    string filePath,
    string validationType,
    int cacheDays = 7)
{
    // Pure file parsing - no AI involved
    var metadata = await ParseBottomMetadata(filePath);
    
    if (metadata.Validations.TryGetValue(validationType, out var validation))
    {
        var lastRun = validation.LastRun;
        var daysSinceRun = (DateTime.UtcNow - lastRun).TotalDays;
        
        if (daysSinceRun < cacheDays)
        {
            return new CacheCheckResult
            {
                IsCached = true,
                CachedResult = validation.Outcome,
                DaysSinceRun = daysSinceRun,
                Message = $"Cache valid. Last run: {lastRun:yyyy-MM-dd}"
            };
        }
    }
    
    return new CacheCheckResult
    {
        IsCached = false,
        Message = "No valid cache found. Run validation."
    };
}
```

**Agent prompt using deterministic tool**:

```yaml
---
name: grammar-review-cached
tools: ['check_validation_cache', 'read_file', 'replace_string_in_file']
---

## Process

### Phase 1: Cache Check (DETERMINISTIC - Zero tokens)

1. Call `check_validation_cache`:
   - filePath: target file
   - validationType: "grammar"
   - cacheDays: 7

2. **IF cache is valid** â†’ Report cached result â†’ EXIT
3. **IF no cache** â†’ Proceed to Phase 2

### Phase 2: Grammar Validation (AI - Consumes tokens)

Only reached if cache miss. Perform full validation...
```

### Token savings from deterministic cache checks

| Scenario | Without Deterministic Check | With Deterministic Check |
|----------|----------------------------|-------------------------|
| Cache hit (70% of cases) | 3,000 tokens | 0 tokens |
| Cache miss (30% of cases) | 3,000 tokens | 3,000 tokens |
| **Expected tokens per run** | 3,000 tokens | 900 tokens |
| **Savings** | | <mark>**70%**</mark> |

## Common deterministic operations to implement

| Operation | Implementation | Token Savings |
|-----------|---------------|---------------|
| **Metadata parsing** | YAML parser, regex | Avoid AI parsing docs |
| **File existence check** | `File.Exists()` | Avoid AI file searches |
| **Link validation** | HTTP HEAD request | Avoid AI fetching pages |
| **Pattern matching** | Regex engine | Avoid AI text analysis |
| **Schema validation** | JSON Schema validator | Avoid AI structure checks |
| **Diff computation** | Git diff | Avoid AI comparison |
| **Timestamp comparison** | Date arithmetic | Avoid AI date reasoning |

---

# âš¡ Strategy 9: Streaming and parallelization

<mark>**Streaming and parallelization**</mark> don't reduce actual token costs but dramatically improve <mark>perceived latency and throughput</mark>. These techniques are essential for production-quality user experiences.

## Streaming: Progressive output delivery

Streaming delivers tokens as they're generated instead of waiting for the complete response:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STREAMING vs BATCH RESPONSE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  WITHOUT STREAMING:                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Request â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Response  â”‚  â”‚
â”‚  â”‚          [User waits 3-5 seconds seeing nothing...]              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚  WITH STREAMING:                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Request â”€â”€â–¶ Here â”€â”€â–¶ is â”€â”€â–¶ the â”€â”€â–¶ response â”€â”€â–¶ ...             â”‚  â”‚
â”‚  â”‚         [~100ms] [~100ms] [~100ms] [~100ms]                       â”‚  â”‚
â”‚  â”‚         [User sees progress immediately]                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚  PERCEIVED LATENCY: 100ms vs 3-5 seconds                               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### When to use streaming

| âœ… Good Use Cases | âŒ Poor Use Cases |
|-------------------|-------------------|
| User-facing chat interfaces | Background batch processing |
| Code generation with preview | API integrations requiring complete response |
| Documentation generation | Validation pipelines |
| Interactive debugging | Structured JSON output parsing |

## Parallelization: Concurrent independent tasks

When tasks don't depend on each other, process them <mark>**concurrently**</mark> rather than sequentially:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEQUENTIAL vs PARALLEL EXECUTION                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  SEQUENTIAL (Bad):                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ Task 1  â”‚â”€â”€â–¶â”‚ Task 2  â”‚â”€â”€â–¶â”‚ Task 3  â”‚â”€â”€â–¶â”‚ Task 4  â”‚                 â”‚
â”‚  â”‚  2 sec  â”‚   â”‚  2 sec  â”‚   â”‚  2 sec  â”‚   â”‚  2 sec  â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚  Total time: 8 seconds                                                  â”‚
â”‚                                                                         â”‚
â”‚  PARALLEL (Good - when tasks are independent):                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                            â”‚
â”‚  â”‚ Task 1  â”‚                                                            â”‚
â”‚  â”‚  2 sec  â”‚                                                            â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                                            â”‚
â”‚  â”‚ Task 2  â”‚     All complete at ~2 seconds                            â”‚
â”‚  â”‚  2 sec  â”‚                                                            â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                                            â”‚
â”‚  â”‚ Task 3  â”‚                                                            â”‚
â”‚  â”‚  2 sec  â”‚                                                            â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                                            â”‚
â”‚  â”‚ Task 4  â”‚                                                            â”‚
â”‚  â”‚  2 sec  â”‚                                                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                            â”‚
â”‚  Total time: ~2 seconds (4Ã— faster)                                    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation: Parallel validation

```csharp
// Validate multiple articles in parallel
public async Task<ValidationReport[]> ValidateArticles(string[] articlePaths)
{
    var validationTasks = articlePaths.Select(async path =>
    {
        var content = await File.ReadAllTextAsync(path);
        
        // These validations can run in parallel
        var grammarTask = ValidateGrammarAsync(content);
        var readabilityTask = ValidateReadabilityAsync(content);
        var structureTask = ValidateStructureAsync(content);
        var linksTask = ValidateLinksAsync(content);
        
        await Task.WhenAll(grammarTask, readabilityTask, structureTask, linksTask);
        
        return new ValidationReport
        {
            Path = path,
            Grammar = await grammarTask,
            Readability = await readabilityTask,
            Structure = await structureTask,
            Links = await linksTask
        };
    });
    
    return await Task.WhenAll(validationTasks);
}
```

## Speculative execution: Predicted outputs

OpenAI's <mark>**Predicted Outputs**</mark> feature reduces latency when you know most of the expected output:

```python
from openai import OpenAI

client = OpenAI()

# When editing code, most of the file stays the same
code = """
def calculate_total(items):
    total = 0
    for item in items:
        total += item.price
    return total
"""

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{
        "role": "user",
        "content": f"Add tax calculation to this function:\n{code}"
    }],
    prediction={
        "type": "content",
        "content": code  # Most of this content will be in output
    }
)

# Latency reduced by ~2-3Ã— because unchanged tokens 
# are confirmed rather than regenerated
```

### When speculative execution helps

| Scenario | Latency Reduction | Best Model |
|----------|-------------------|------------|
| Code refactoring (small changes) | 2-3Ã— | gpt-4o, gpt-4o-mini |
| Document editing | 2Ã— | gpt-4o |
| Template completion | 3-4Ã— | gpt-4o-mini |
| Translation with preserved formatting | 2Ã— | gpt-4o |

## Combined latency optimization

A well-optimized user-facing workflow combines these techniques:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OPTIMAL LATENCY ARCHITECTURE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  User Request                                                           â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ PARALLEL SETUP (concurrent)                                      â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚ â”‚ Check cache  â”‚ â”‚ Fetch contextâ”‚ â”‚ Load templatesâ”‚              â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STREAMING RESPONSE                                               â”‚   â”‚
â”‚  â”‚ User sees tokens appear immediately (~100ms first token)         â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚ If editing existing content: use PREDICTED OUTPUTS               â”‚   â”‚
â”‚  â”‚ for 2-3Ã— additional latency reduction                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ PARALLEL POST-PROCESSING (while user sees response)             â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚ â”‚ Update cache â”‚ â”‚ Log metrics  â”‚ â”‚ Pre-warm nextâ”‚              â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  RESULT: Sub-second perceived latency                                  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ“ˆ Strategy comparison matrix

| Strategy | Token Savings | Implementation Effort | Best For | Limitations |
|----------|--------------|----------------------|----------|-------------|
| **<mark>1. Context Reduction</mark>** | 30-90% | Low | All workflows | Requires discipline |
| **<mark>2. Provider Caching</mark>** | 50-90% (stable prefixes) | Low (structure prompts correctly) | High-volume, consistent prompts | Minimum token requirements |
| **<mark>3. Semantic Caching</mark>** | 50-80% (high hit scenarios) | High (embedding infrastructure) | Repeated queries, documentation | Stale data risk, false positives |
| **<mark>4. Model Selection</mark>** | 60-80% | Low | Simple tasks, high volume | May reduce quality for complex tasks |
| **<mark>5. Batch Processing</mark>** | 50% cost | Low | Non-urgent, high volume | 24h latency |
| **<mark>6. Request Consolidation</mark>** | 40-60% | Medium | Multi-step pipelines | Increased prompt complexity |
| **<mark>7. Output Reduction</mark>** | 30-50% | Low | Verbose outputs | May lose useful detail |
| **<mark>8. Deterministic Tools</mark>** | 70-100% | Medium (MCP development) | Cache checks, validation, file ops | Limited to predictable operations |
| **<mark>9. Streaming/Parallelization</mark>** | Latency only | Low-Medium | User-facing, independent tasks | No token savings |

## Strategy selection flowchart

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OPTIMIZATION STRATEGY SELECTION                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Is the operation predictable/deterministic?
â”œâ”€â”€ YES â†’ Use DETERMINISTIC TOOL (zero tokens)
â”‚         Examples: cache checks, file existence, schema validation
â”‚
â””â”€â”€ NO â†’ Is it a repeated query pattern?
         â”œâ”€â”€ YES â†’ Is the prefix stable?
         â”‚         â”œâ”€â”€ YES â†’ Use PROVIDER CACHING (90% savings)
         â”‚         â”‚         Structure: static first, dynamic last
         â”‚         â”‚
         â”‚         â””â”€â”€ NO â†’ Consider SEMANTIC CACHING (50-80% savings)
         â”‚                  If query variations are semantically similar
         â”‚
         â””â”€â”€ NO â†’ Is it high volume, non-urgent?
                  â”œâ”€â”€ YES â†’ Use BATCH PROCESSING (50% discount)
                  â”‚
                  â””â”€â”€ NO â†’ Is output verbose?
                           â”œâ”€â”€ YES â†’ Use OUTPUT REDUCTION (30-50% savings)
                           â”‚
                           â””â”€â”€ NO â†’ Use CONTEXT REDUCTION (30-90% savings)
                                    â€¢ Targeted file reads
                                    â€¢ Search result limits
                                    â€¢ Progressive summarization
```

## Combined optimization example

A well-optimized 5-phase workflow uses <mark>**multiple strategies in combination**</mark>:

```
Phase 1: Research
â”œâ”€â”€ MODEL SELECTION: Use GPT-4o-mini for initial search
â”œâ”€â”€ CONTEXT REDUCTION: Limit search to 5 results
â”œâ”€â”€ PROVIDER CACHING: Stable research prompt prefix
â””â”€â”€ Expected: 70% savings

Phase 2: Cache Check
â”œâ”€â”€ DETERMINISTIC TOOL: Check existing validation cache
â”œâ”€â”€ If cache hit: Skip remaining phases
â””â”€â”€ Expected: 70% of runs skip AI entirely

Phase 3: Analysis (cache miss only)
â”œâ”€â”€ CONTEXT REDUCTION: Read only relevant sections
â”œâ”€â”€ PROVIDER CACHING: Stable analysis prompt
â”œâ”€â”€ REQUEST CONSOLIDATION: Combine grammar + readability checks
â””â”€â”€ Expected: 60% savings

Phase 4: Generation
â”œâ”€â”€ CONTEXT REDUCTION: Progressive summary from Phase 3
â”œâ”€â”€ SEMANTIC CACHING: Cache common generation patterns
â”œâ”€â”€ OUTPUT REDUCTION: Structured JSON output only
â””â”€â”€ Expected: 50% savings

Phase 5: Validation
â”œâ”€â”€ DETERMINISTIC TOOL: Schema validation, link checks
â”œâ”€â”€ BATCH PROCESSING: Queue non-urgent validations
â”œâ”€â”€ Only AI for: Grammar, readability, semantic checks
â””â”€â”€ Expected: 70% of checks bypass AI

CUMULATIVE SAVINGS: 75-90%
```

---

# ğŸ”§ Implementation patterns

## Pattern 1: Validation pipeline with caching

```yaml
# validation-pipeline.prompt.md
---
name: validation-pipeline
description: "Multi-validation with deterministic cache checks"
tools: ['check_validation_cache', 'run_grammar_check', 'run_readability_check']
---

## Process

### Step 1: Batch Cache Check (DETERMINISTIC)

For each validation type (grammar, readability, structure, fact-check):
1. Call `check_validation_cache(file, type, days=7)`
2. Record which validations need running

### Step 2: Run Only Missing Validations (AI)

For each validation NOT in cache:
1. Run appropriate validation prompt
2. Store result in metadata cache

### Step 3: Aggregate Results

Combine cached + fresh results into unified report.
```

## Pattern 2: Research with semantic caching

```python
# Research pattern with semantic cache
async def research_topic(topic: str, cache: SemanticCache):
    # Check semantic cache first
    cached = await cache.get_similar(topic)
    if cached and cached.similarity > 0.90:
        return cached.response
    
    # Cache miss - perform research
    results = await perform_research(topic)
    
    # Store for future similar queries
    await cache.store(topic, results)
    
    return results
```

## Pattern 3: Progressive summarization handoff

```yaml
# builder.agent.md
---
name: builder
handoffs:
  - label: "Validate Result"
    agent: validator
    send: false    # Don't send full context
    prompt: |
      **Summary from Builder:**
      {{PHASE_SUMMARY}}
      
      **Artifact location:** {{OUTPUT_FILE}}
      
      Validate the created artifact.
---

## Phase Completion Instructions

Before any handoff, produce a PHASE_SUMMARY (max 200 tokens):

1. Decisions made (bullet list)
2. Artifacts created (file paths)
3. Key constraints applied
4. Specific validation needs

Store full details in output file for reference if needed.
```

---

# âš ï¸ Common pitfalls

## Pitfall 1: Over-caching dynamic content

âŒ **Wrong**: Caching responses that depend on current file state

```python
# DON'T cache file-dependent analyses
cache.store(
    "analyze security of auth.py",  # Query seems cacheable...
    analysis_result  # But result depends on file content!
)
```

âœ… **Right**: Include content hash in cache key

```python
content_hash = hashlib.md5(file_content.encode()).hexdigest()
cache.store(
    f"analyze security of auth.py:{content_hash}",
    analysis_result
)
```

## Pitfall 2: Cache key collisions

âŒ **Wrong**: Overly broad cache keys

```python
cache.store("validate article", result)  # Which article?
```

âœ… **Right**: Include all relevant context in key

```python
cache.store(f"validate:{file_path}:{validation_type}:{content_hash}", result)
```

## Pitfall 3: Ignoring cache write costs

For Anthropic, cache writes cost <mark>**25% more**</mark> than regular input tokens.

âŒ **Wrong**: Caching tiny prefixes that are rarely reused

âœ… **Right**: Only cache prefixes that will be reused 3+ times

```
Break-even calculation (Anthropic):
- Cache write: 1.25Ã— base cost
- Cache read: 0.1Ã— base cost

To save money: Need 2+ cache hits to offset write cost
  1.25 (write) + 0.1 (read) + 0.1 (read) = 1.45
  vs. 1.0 + 1.0 + 1.0 = 3.0 without caching

Savings start at 3rd use.
```

## Pitfall 4: Placing dynamic content before static

âŒ **Wrong**: User input first

```markdown
## User Request: {{input}}

## Instructions (static)
[These won't be cached because they come after dynamic content]
```

âœ… **Right**: Static first, dynamic last

```markdown
## Instructions (static - cached)
[1,000+ tokens of stable content]

## User Request: {{input}}
```

---

# ğŸ¯ Conclusion

Token optimization isn't optional for production AI workflowsâ€”it's the difference between sustainable and unsustainable costs.

## Key takeaways

1. **<mark>Context reduction</mark>** is the foundation: targeted reads, limited searches, progressive summarization
2. **<mark>Provider caching</mark>** offers up to 90% savingsâ€”structure prompts with static content first
3. **<mark>Semantic caching</mark>** captures similar queriesâ€”powerful for documentation and reference lookups
4. **<mark>Model selection</mark>** and **batch processing** provide substantial cost reduction for high-volume workflows
5. **<mark>Request consolidation</mark>** and **output reduction** minimize token counts per interaction
6. **<mark>Deterministic tools</mark>** bypass AI entirely for predictable operationsâ€”cache checks, validation, file operations
7. **<mark>Streaming/parallelization</mark>** improve perceived latency without changing token costs

## Implementation priority

| Priority | Strategy | Expected Savings | Effort |
|----------|----------|-----------------|--------|
| 1 | Context reduction (targeted reads) | 30-50% | Low |
| 2 | Provider caching (prompt structure) | 50-90% | Low |
| 3 | Model selection (right-size tasks) | 60-80% | Low |
| 4 | Batch processing (async high-volume) | 50% cost | Low |
| 5 | Output reduction (structured output) | 30-50% | Low |
| 6 | Request consolidation (combine steps) | 40-60% | Medium |
| 7 | Deterministic tools (cache checks) | 70%+ for cached ops | Medium |
| 8 | Semantic caching | 50-80% | High |
| 9 | Streaming/parallelization | Latency only | Low-Medium |

## Next steps

- **Audit current prompts** for context reduction opportunities
- **Restructure prompts** for provider caching (static first)
- **Evaluate model selection** for different task complexities
- **Identify batch opportunities** for non-urgent high-volume tasks
- **Identify deterministic operations** to move to MCP tools
- **Monitor token usage** to validate savings

For information flow patterns between phases, see the previous article: [How to Manage Information Flow During Prompt Orchestrations](./09.00-how_to_manage_information_flow_during_prompt_orchestrations.md).

---

# ğŸ“š References

## Official Documentation

**[OpenAI Latency Optimization Guide](https://platform.openai.com/docs/guides/latency-optimization)** ğŸ“˜ [Official]  
Seven principles for optimizing latency: process tokens faster, generate fewer tokens, use fewer input tokens, make fewer requests, parallelize, make users wait less, and don't default to LLM.

**[OpenAI Prompt Caching Guide](https://platform.openai.com/docs/guides/prompt-caching)** ğŸ“˜ [Official]  
Comprehensive guide to OpenAI's automatic prompt caching, including requirements (1024+ tokens), cache duration, and best practices for structuring prompts to maximize cache hits.

**[Anthropic Prompt Caching Documentation](https://platform.claude.com/docs/en/docs/build-with-claude/prompt-caching)** ğŸ“˜ [Official]  
Comprehensive guide to Claude's prompt caching with `cache_control` breakpoints, 90% read discount, 25% write premium, 5-minute default or 1-hour extended TTL, up to 4 cache breakpoints, and model-specific minimum token requirements (1,024-4,096 tokens).

**[OpenAI Batch API Guide](https://platform.openai.com/docs/guides/batch)** ğŸ“˜ [Official]  
Documentation for OpenAI's Batch API offering 50% cost discount for async processing with 24-hour completion window and up to 50,000 requests.

**[Anthropic Message Batches API](https://platform.claude.com/docs/en/docs/build-with-claude/batch-processing)** ğŸ“˜ [Official]  
Documentation for Anthropic's batch processing with 50% cost discount, 100,000 request limit (or 256 MB size limit), typically under 1-hour completion, and 29-day result retention.

**[OpenAI Predicted Outputs](https://platform.openai.com/docs/guides/predicted-outputs)** ğŸ“˜ [Official]  
Guide to using predicted outputs for speculative execution, reducing latency by 2-3Ã— when output is largely known in advance.

## Community Resources

**[GPTCache Documentation](https://gptcache.readthedocs.io/)** ğŸ“— [Verified Community]  
Open-source semantic caching library by Zilliz. Provides embedding-based similarity matching to cache LLM responses for similar queries, with support for multiple vector stores and embedding providers.

## Internal References

**[Validation Caching Pattern](../../.copilot/context/00.00-prompt-engineering/05-validation-caching-pattern.md)** ğŸ“˜ [Internal]  
Repository-specific implementation of the 7-day validation caching pattern using bottom metadata blocks.

**[Tool Composition Guide](../../.copilot/context/00.00-prompt-engineering/02-tool-composition-guide.md)** ğŸ“˜ [Internal]  
Optimization patterns for tool usage including "narrow before wide" and lazy loading strategies.

**[Context Engineering Principles](../../.copilot/context/00.00-prompt-engineering/01-context-engineering-principles.md)** ğŸ“˜ [Internal]  
Token budget guidelines and context window management principles.

## Series Navigation

**Previous**: [How to Manage Information Flow During Prompt Orchestrations](./09.00-how_to_manage_information_flow_during_prompt_orchestrations.md)  
**Series Index**: [How GitHub Copilot Uses Markdown and Prompt Folders](./02-getting-started/01.00-how_github_copilot_uses_markdown_and_prompt_folders.md)

---

<!--
validations:
  grammar:
    status: "not_run"
    last_run: null
    model: null
    outcome: null
    issues_found: 0
  
  readability:
    status: "not_run"
    last_run: null
    model: null
    outcome: null
    flesch_score: null
    grade_level: null
  
  understandability:
    status: "not_run"
    last_run: null
    model: null
    outcome: null
    issues_found: 0
  
  structure:
    status: "not_run"
    last_run: null
    model: null
    outcome: null
    missing_sections: []
  
  fact_check:
    status: "complete"
    last_run: "2026-01-26"
    model: "Claude Opus 4.5"
    outcome: "pass"
    claims_verified: 7
    sources_checked: 7
    notes: "All 7 external references verified. Updated 2 Anthropic URLs (docs.anthropic.com â†’ platform.claude.com). Added model-specific token minimums, 1-hour cache TTL details, cache invalidation patterns, and batch caching best-effort note."
  
  logic:
    status: "not_run"
    last_run: null
    model: null
    outcome: null
    issues_found: 0

article_metadata:
  filename: "10.00-how_to_optimize_token_consumption_during_prompt_orchestrations.md"
  created: "2026-01-25"
  last_updated: "2026-01-26"
  word_count: ~5800
  reading_time: "23 min"
  
series:
  name: "Prompt Engineering for GitHub Copilot"
  position: 10
  previous: "09.00-how_to_manage_information_flow_during_prompt_orchestrations.md"
  
cross_references:
  - path: "./09.00-how_to_manage_information_flow_during_prompt_orchestrations.md"
    relationship: "predecessor"
    context: "Covers information flow; this article optimizes token consumption"
  - path: ".copilot/context/00.00-prompt-engineering/05-validation-caching-pattern.md"
    relationship: "implements"
    context: "Deterministic caching pattern referenced in Strategy 8"
  - path: ".copilot/context/00.00-prompt-engineering/02-tool-composition-guide.md"
    relationship: "extends"
    context: "Tool optimization patterns for context reduction"
-->
